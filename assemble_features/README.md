
# README for Lidar Processing Workflow

## Overview

This repository contains a Python-based workflow for processing LIDAR data. The script handles several tasks such as merging `.las` files, compressing them to `.laz`, classifying point clouds, extracting ground and canopy data, and generating vegetation height TIFF files. The final output is stored in the specified output directory, and temporary files are cleaned up after processing.

The workflow includes multiple stages, each executed in sequence, with checks to skip stages if the results already exist. The process is capable of handling different resolutions for the output TIFF files and ensures that gaps in data (no-data pixels) are filled where necessary.

## Requirements

### Python Dependencies
This project requires the following libraries:
- `os`
- `pathlib`
- `logging`
- `shutil`

Additionally, the following external modules are imported:
- `libs.classify_point_cloud`
- `libs.compress_las_to_laz`
- `libs.extract_canopy`
- `libs.extract_ground`
- `libs.extract_veg_height`
- `libs.merge_las_files`
- `libs.merge_laz`
- `libs.split_laz_file`
- `libs.misc`
- `libs.tiff_postprocessing`

The script assumes these modules handle specific LIDAR processing tasks, such as point cloud classification, compression, and merging. Make sure these libraries are available in the `libs` folder and properly configured.

### Configuration
- **TIFF_RESOLUTIONS**: This is imported from a configuration file `config.py`. It should be an iterable defining different resolutions for the TIFF outputs (e.g., 1m, 5m, 10m).

### External Tools
The script assumes the presence of the `PROJ_LIB` library at `/opt/conda/share/proj` to handle geospatial transformations. Ensure the environment variable is correctly set.

## Directory Structure

The script expects the following directory structure:

```
/input         - Directory containing input `.las` files.
/output        - Directory for storing the final output files.
/dev/shm/lidar-part - Temporary directory for intermediate files.
```

## Workflow

### Main Processing Steps
1. **Merge `.las` Files**:
   The `.las` files in the `/input` directory are merged into a single file using `merge_las_files`.

2. **Compress to `.laz`**:
   The merged `.las` file is compressed into `.laz` format using `compress_las_to_laz`.

3. **Classify Point Cloud**:
   The `.laz` file is split into smaller parts, and the points are classified (e.g., ground, vegetation) using `classify_point_cloud`. The classified parts are then merged back into a single `.laz` file.

4. **Extract Ground, Canopy, and Vegetation Height**:
   For each resolution in `TIFF_RESOLUTIONS`, the classified `.laz` file is used to extract:
   - Ground data
   - Canopy data
   - Vegetation height
   
   Each of these data types is saved as TIFF files, with gaps in the data (no-data pixels) filled in.

5. **Cleanup**:
   After processing, the temporary directory `/dev/shm/lidar-part` is removed to free up disk space.

### Error Handling
The workflow checks if files already exist before running each step, allowing for efficient reprocessing or resuming interrupted runs.

## Usage

1. Place your `.las` input files in the `/input` directory.
2. Ensure the environment is correctly configured with the necessary Python packages and the `libs` folder.
3. Run the main script:

    ```bash
    python main.py
    ```

4. Output files will be generated in the `/output` directory.

## Output Files

- **01-compressed.laz**: The compressed version of the merged `.las` files.
- **02-classified_points.laz**: The classified `.laz` file after point cloud classification.
- **02-ground.tif**: Ground surface data extracted from the classified `.laz`.
- **03-canopy.tif**: Canopy height data extracted from the classified `.laz`.
- **04-veg_height.tif**: Vegetation height data extracted from the classified `.laz`.

Each of these TIFF files will have multiple versions based on the resolutions defined in `TIFF_RESOLUTIONS`. Additionally, no-data gaps are filled using a specified window size.

## Notes
- Ensure you have sufficient disk space, especially for the temporary files in `/dev/shm/lidar-part`.
- The logging is set to output all messages in DEBUG mode, which provides detailed information on the workflow's progress.

---

This README provides the necessary information to understand and run the LIDAR processing workflow. If any issues arise, consult the log files for more details or check that all necessary external libraries and configurations are in place.
This README was generated by ChatGPT based on the code structure. 
